{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition and MNist \n",
    "#### I built a neural network to recognize handwritten digits using keras and it has a tensor flow as our backend. \n",
    "#### I created a simple neural network to predict, as accurately as we can, digits from handwritten images.\n",
    "#### There is an input layer, and the neurons within the layer are called input neurons. Then we have an output layer contains the output neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Handwritten Digit Rec](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/05/Examples-from-the-MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports i need for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abiga\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import keras as kr\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as pre\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip the files\n",
    "#### I used gzip to unzip all the training, testing images and labels. #### These  dataset contains 60,000 training images and 10,000 testing images and reads them from your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_images = f.read()\n",
    "    \n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_labels = f.read()\n",
    "    \n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    training_images = f.read()\n",
    "    \n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    training_labels = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will read all the files and save them into memory\n",
    "#### Allows the files to be saved as png to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = ~np.array(list(training_images[16:])).reshape(60000, 28, 28).astype(np.uint8) / 255.0\n",
    "training_labels =  np.array(list(training_labels[8:])).astype(np.uint8)\n",
    "\n",
    "test_images = ~np.array(list(test_images[16:])).reshape(10000, 784).astype(np.uint8) / 255.0\n",
    "test_labels = np.array(list(test_labels[8:])).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The inputs and outputs used in the training set for images and labels. It goes through the 10000 testing sets\n",
    "#### The binary format is a 10 x 10 matrix. Firstly we to setup the matrix using the labelBinerizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98823529, 0.92941176, 0.92941176,\n",
       "        0.92941176, 0.50588235, 0.46666667, 0.31372549, 0.89803922,\n",
       "        0.34901961, 0.        , 0.03137255, 0.50196078, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.88235294, 0.85882353, 0.63137255, 0.39607843,\n",
       "        0.33333333, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.11764706, 0.3254902 , 0.00784314, 0.05098039,\n",
       "        0.23529412, 0.74901961, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.80784314, 0.06666667,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.01568627, 0.63529412,\n",
       "        0.67843137, 0.67843137, 0.78039216, 0.84705882, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92941176, 0.14117647, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.22352941, 0.28627451,\n",
       "        0.03137255, 0.05490196, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.68627451, 0.38823529, 0.58039216, 0.00784314, 0.00784314,\n",
       "        0.19607843, 0.95686275, 1.        , 0.83137255, 0.39607843,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.94509804,\n",
       "        0.99607843, 0.39607843, 0.00784314, 0.64705882, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.45490196,\n",
       "        0.00784314, 0.25490196, 0.99215686, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.95686275, 0.25490196, 0.00784314,\n",
       "        0.7254902 , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.8627451 , 0.05490196, 0.11764706, 0.37254902,\n",
       "        0.57647059, 0.99607843, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.68235294, 0.05882353, 0.00784314, 0.00784314, 0.53333333,\n",
       "        0.90196078, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.82352941,\n",
       "        0.27058824, 0.00784314, 0.00784314, 0.41176471, 0.89411765,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.9372549 , 0.63529412,\n",
       "        0.01176471, 0.00784314, 0.26666667, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.02352941, 0.00784314,\n",
       "        0.02352941, 0.74901961, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.81960784, 0.49019608,\n",
       "        0.28235294, 0.00784314, 0.00784314, 0.18823529, 0.99215686,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.84705882,\n",
       "        0.41960784, 0.10196078, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.01960784, 0.28627451, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.90588235, 0.55294118, 0.13333333, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.21176471, 0.69411765, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.90980392, 0.74117647, 0.16470588, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.22352941, 0.68235294,\n",
       "        0.99215686, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.92941176, 0.32941176, 0.14117647,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.23529412,\n",
       "        0.68627451, 0.96470588, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.78431373, 0.3254902 ,\n",
       "        0.11372549, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.04313725, 0.47843137, 0.95686275, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.46666667, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.16862745, 0.47058824, 0.48235294, 0.9372549 , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the array , 784 neurons\n",
    "#part of the array printed below.\n",
    "inputs = training_images.reshape(60000, 784)\n",
    "inputs[0:1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the data into binary values\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(training_labels)\n",
    "outputs = encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network \n",
    "![](https://i.stack.imgur.com/Kc50L.jpg)\n",
    "#### It has 3 layers in the network.The activation function set as ReLu. To determine which class to output, we will rely on the SoftMax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build our neural network model\n",
    "# Adapted from https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "# 5 layers - input layer, 3 hidden layer and 1 output layer\n",
    "model = Sequential()\n",
    "\n",
    "# Simple Neural Network with 3 layers (750, 512, 200)\n",
    "model.add(kr.layers.Dense(units=750, activation='relu', input_dim=784))\n",
    "model.add(kr.layers.Dense(units=512, activation='relu'))\n",
    "model.add(kr.layers.Dense(units=200, activation='relu'))\n",
    "# Compile model - Adam optimizer for our model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Add 10 output neurons, one for each\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 750)               588750    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               384512    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 1,075,862\n",
      "Trainable params: 1,075,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abiga\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "# My layers input , output and hidden layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training set files through the 60000 images and gives best accuracy\n",
    "#### We run 20 epochs with updates every 100 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. \n",
    "#### A verbose value of 2 is used to reduce the output to one line for each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 27s 458us/step - loss: 0.0787 - acc: 0.9722\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 27s 448us/step - loss: 0.0415 - acc: 0.98552\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 23s 381us/step - loss: 0.0314 - acc: 0.98911s - loss: 0.\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 25s 410us/step - loss: 0.0271 - acc: 0.9906\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 21s 348us/step - loss: 0.0230 - acc: 0.99205s - loss - ETA: 3s - loss:  - ETA: 2\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 21s 351us/step - loss: 0.0217 - acc: 0.99256s - loss - \n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 25s 418us/step - loss: 0.0201 - acc: 0.99291s - los - ETA: 0s - loss\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 29s 484us/step - loss: 0.0176 - acc: 0.9939\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 28s 462us/step - loss: 0.0166 - acc: 0.9942\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 23s 384us/step - loss: 0.0160 - acc: 0.9944\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 21s 355us/step - loss: 0.0151 - acc: 0.9947\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 18s 302us/step - loss: 0.0137 - acc: 0.9953\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 43s 716us/step - loss: 0.0126 - acc: 0.9956\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 37s 611us/step - loss: 0.0124 - acc: 0.9957\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 38s 637us/step - loss: 0.0118 - acc: 0.9959\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 33s 551us/step - loss: 0.0110 - acc: 0.9961\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 36s 605us/step - loss: 0.0114 - acc: 0.9959 - ETA: 1s - loss:\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 36s 602us/step - loss: 0.0097 - acc: 0.9965\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 33s 547us/step - loss: 0.0100 - acc: 0.9964\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 32s 535us/step - loss: 0.0092 - acc: 0.9968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1615699bb38>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train - The model is going to fit over 20 epochs \n",
    "#and updates after every 100 images training.\n",
    "\n",
    "model.fit(inputs, outputs, epochs=20, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the accuracy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics(Test loss & Test Accuracy): \n",
      "[0.008199873295654348, 0.9970966615358988]\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(inputs, outputs, verbose=0)\n",
    "print ('Metrics(Test loss & Test Accuracy): ')\n",
    "\n",
    "print (metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prints the error rate the lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 0.29%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(inputs, outputs, verbose=2)\n",
    "print('Error Rate: %.2f%%' % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refernces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Accuracy Error](https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/),\n",
    "[Network Layers](https://www.kaggle.com/ngbolin/mnist-dataset-digit-recognizer),\n",
    "[Neural network](http://neuralnetworksanddeeplearning.com/chap1.html),\n",
    "[Loading the model](https://medium.com/coinmonks/handwritten-digit-prediction-using-convolutional-neural-networks-in-tensorflow-with-keras-and-live-5ebddf46dc8),\n",
    "[Predictions](https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-recognize-handwritten-digits-with-tensorflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
