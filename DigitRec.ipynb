{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition and MNist \n",
    "#### I built a neural network to recognize handwritten digits using keras and it has a tensor flow as our backend. \n",
    "#### I created a simple neural network to predict, as accurately as we can, digits from handwritten images.\n",
    "#### There is an input layer, and the neurons within the layer are called input neurons. Then we have an output layer contains the output neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Handwritten Digit Rec](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/05/Examples-from-the-MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports i need for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abiga\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import keras as kr\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as pre\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip the files\n",
    "#### I used gzip to unzip all the training, testing images and labels.\n",
    "#### These  dataset contains 60,000 training images and 10,000 testing images and reads them from your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_images = f.read()\n",
    "    \n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_labels = f.read()\n",
    "    \n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    training_images = f.read()\n",
    "    \n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    training_labels = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will read all the files and save them into memory\n",
    "#### Allows the files to be saved as png to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = ~np.array(list(training_images[16:])).reshape(60000, 28, 28).astype(np.uint8) / 255.0\n",
    "training_labels =  np.array(list(training_labels[8:])).astype(np.uint8)\n",
    "\n",
    "test_images = ~np.array(list(test_images[16:])).reshape(10000, 784).astype(np.uint8) / 255.0\n",
    "test_labels = np.array(list(test_labels[8:])).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The inputs and outputs used in the training set for images and labels. It goes through the 10000 testing sets\n",
    "#### The binary format is a 10 x 10 matrix. Firstly we to setup the matrix using the labelBinerizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98823529, 0.92941176, 0.92941176,\n",
       "        0.92941176, 0.50588235, 0.46666667, 0.31372549, 0.89803922,\n",
       "        0.34901961, 0.        , 0.03137255, 0.50196078, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.88235294, 0.85882353, 0.63137255, 0.39607843,\n",
       "        0.33333333, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.11764706, 0.3254902 , 0.00784314, 0.05098039,\n",
       "        0.23529412, 0.74901961, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.80784314, 0.06666667,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.01568627, 0.63529412,\n",
       "        0.67843137, 0.67843137, 0.78039216, 0.84705882, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92941176, 0.14117647, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.22352941, 0.28627451,\n",
       "        0.03137255, 0.05490196, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.68627451, 0.38823529, 0.58039216, 0.00784314, 0.00784314,\n",
       "        0.19607843, 0.95686275, 1.        , 0.83137255, 0.39607843,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.94509804,\n",
       "        0.99607843, 0.39607843, 0.00784314, 0.64705882, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.45490196,\n",
       "        0.00784314, 0.25490196, 0.99215686, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.95686275, 0.25490196, 0.00784314,\n",
       "        0.7254902 , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.8627451 , 0.05490196, 0.11764706, 0.37254902,\n",
       "        0.57647059, 0.99607843, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.68235294, 0.05882353, 0.00784314, 0.00784314, 0.53333333,\n",
       "        0.90196078, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.82352941,\n",
       "        0.27058824, 0.00784314, 0.00784314, 0.41176471, 0.89411765,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.9372549 , 0.63529412,\n",
       "        0.01176471, 0.00784314, 0.26666667, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.02352941, 0.00784314,\n",
       "        0.02352941, 0.74901961, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.81960784, 0.49019608,\n",
       "        0.28235294, 0.00784314, 0.00784314, 0.18823529, 0.99215686,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.84705882,\n",
       "        0.41960784, 0.10196078, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.01960784, 0.28627451, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.90588235, 0.55294118, 0.13333333, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.21176471, 0.69411765, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.90980392, 0.74117647, 0.16470588, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.22352941, 0.68235294,\n",
       "        0.99215686, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.92941176, 0.32941176, 0.14117647,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.23529412,\n",
       "        0.68627451, 0.96470588, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.78431373, 0.3254902 ,\n",
       "        0.11372549, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.04313725, 0.47843137, 0.95686275, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.46666667, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.16862745, 0.47058824, 0.48235294, 0.9372549 , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the array , 784 neurons\n",
    "#part of the array printed below.\n",
    "inputs = training_images.reshape(60000, 784)\n",
    "inputs[0:1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the data into binary values\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(training_labels)\n",
    "outputs = encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network \n",
    "![](https://i.stack.imgur.com/Kc50L.jpg)\n",
    "#### It has 3 layers in the network.The activation function set as ReLu. To determine which class to output, we will rely on the SoftMax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build our neural network model\n",
    "# Adapted from https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "# 5 layers - input layer, 3 hidden layer and 1 output layer\n",
    "model = Sequential()\n",
    "\n",
    "# Simple Neural Network with 3 layers (750, 512, 200)\n",
    "model.add(kr.layers.Dense(units=750, activation='relu', input_dim=784))\n",
    "model.add(kr.layers.Dense(units=512, activation='relu'))\n",
    "model.add(kr.layers.Dense(units=200, activation='relu'))\n",
    "# Compile model - Adam optimizer for our model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Add 10 output neurons, one for each\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 750)               588750    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               384512    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 1,075,862\n",
      "Trainable params: 1,075,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abiga\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "# My layers input , output and hidden layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training set files through the 60000 images and gives best accuracy\n",
    "#### We run 20 epochs with updates every 100 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. \n",
    "#### A verbose value of 2 is used to reduce the output to one line for each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 21s 348us/step - loss: 0.0781 - acc: 0.9722\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 20s 339us/step - loss: 0.0413 - acc: 0.9854\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 20s 340us/step - loss: 0.0330 - acc: 0.9886\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 21s 352us/step - loss: 0.0284 - acc: 0.99013s  - ETA: 1s - loss:\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 22s 370us/step - loss: 0.0243 - acc: 0.99161s - loss: 0.0246\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 0.0221 - acc: 0.9923\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.0205 - acc: 0.9930\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 20s 334us/step - loss: 0.0185 - acc: 0.9935\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 19s 325us/step - loss: 0.0171 - acc: 0.9941\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.0161 - acc: 0.9946\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 20s 341us/step - loss: 0.0152 - acc: 0.9947\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.0137 - acc: 0.99534s - l\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 0.0131 - acc: 0.9955\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 21s 347us/step - loss: 0.0124 - acc: 0.99570s - loss: 0.0124 - acc: 0.9\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 19s 322us/step - loss: 0.0120 - acc: 0.9959\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 19s 316us/step - loss: 0.0113 - acc: 0.9960\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 19s 319us/step - loss: 0.0111 - acc: 0.9961\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 19s 319us/step - loss: 0.0098 - acc: 0.9965\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 19s 317us/step - loss: 0.0104 - acc: 0.9963\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 0.0082 - acc: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b0f0299208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train - The model is going to fit over 20 epochs \n",
    "#and updates after every 100 images training.\n",
    "\n",
    "model.fit(inputs, outputs, epochs=20, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the accuracy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics(Test loss & Test Accuracy): \n",
      "[0.010707166688031672, 0.9961033282279969]\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(inputs, outputs, verbose=0)\n",
    "print ('Metrics(Test loss & Test Accuracy): ')\n",
    "\n",
    "print (metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prints the error rate the lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 0.39%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(inputs, outputs, verbose=2)\n",
    "print('Error Rate: %.2f%%' % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9696\n"
     ]
    }
   ],
   "source": [
    "print((encoder.inverse_transform(model.predict(test_images)) == test_labels).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refernces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Accuracy Error](https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/),\n",
    "[Network Layers](https://www.kaggle.com/ngbolin/mnist-dataset-digit-recognizer),\n",
    "[Neural network](http://neuralnetworksanddeeplearning.com/chap1.html),\n",
    "[Loading the model](https://medium.com/coinmonks/handwritten-digit-prediction-using-convolutional-neural-networks-in-tensorflow-with-keras-and-live-5ebddf46dc8),\n",
    "[Predictions](https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-recognize-handwritten-digits-with-tensorflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
